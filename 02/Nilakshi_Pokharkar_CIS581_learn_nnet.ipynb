{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRxLDHxHazy5"
   },
   "source": [
    "Neural Network Code\n",
    "\n",
    "CIS 481/581 Computational Learning\n",
    "\n",
    "Instructor: Luis E Ortiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hJCF_G25iyUY"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import ParameterGrid, KFold, train_test_split, cross_val_score\n",
    "\n",
    "from numpy.random import default_rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pkhpFcK9u-Bl"
   },
   "outputs": [],
   "source": [
    "rng = default_rng()\n",
    "\n",
    "def random_uniform(n,m,R=[-1.0,1.0]):\n",
    "    a, b = R[0], R[1]\n",
    "    return (b - a) * rng.random((n,m)) + a\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def deriv_sigmoid(x):\n",
    "    return sigmoid(x) * (1.0 - sigmoid(x))\n",
    "\n",
    "def squared_error(y_true,y_pred):\n",
    "    return 0.5 * (y_true - y_pred) ** 2\n",
    "\n",
    "def deriv_squared_error(y_true,y_pred):\n",
    "    return y_pred - y_true\n",
    "\n",
    "def indicator(p):\n",
    "    return p.astype(int)\n",
    "\n",
    "def error_rate(y_true, y_pred):\n",
    "    return 1.0 - np.mean(indicator(y_true == y_pred))\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def deriv_identity(x):\n",
    "    return np.ones(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jPsZlG57fMC_"
   },
   "outputs": [],
   "source": [
    "def make_nunits(n,K,L,N): #L is the maximum number of hidden layer, N Maximum hidden layer units\n",
    "    nunits = [n]\n",
    "    for l in range(L):\n",
    "        nunits.append(N)\n",
    "        nunits.append(K)\n",
    "        return nunits\n",
    "\n",
    "def time_nnet(nunits):\n",
    "    t = 0\n",
    "    for l in range(len(nunits)-1):\n",
    "        t += (nunits[l] + 1) * nunits[l+1]\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GU84dElbNmUy"
   },
   "outputs": [],
   "source": [
    "MAX_ITERS = 50\n",
    "MAX_NHIDU = 2**9 #512\n",
    "MAX_NHIDL = 2**2\n",
    "MAX_M = 2000\n",
    "# n= 1024\n",
    "# K=10\n",
    "MAX_NUNITS = make_nunits(n,K,MAX_NHIDL,MAX_NHIDU)\n",
    "MAX_NNET_TIME = time_nnet(MAX_NUNITS)\n",
    "\n",
    "MAX_TIME = MAX_M * MAX_NNET_TIME * MAX_ITERS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wg9AyGDxNrpM"
   },
   "outputs": [],
   "source": [
    "class NNetBaseFunction:\n",
    "    def __init__(self, f=None,df=None):\n",
    "        self.f = f\n",
    "        self.df = df \n",
    "\n",
    "    def deepcopy(self):\n",
    "        return NNetBaseFunction(f=self.f, df=self.df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lyle0ij62JSh"
   },
   "outputs": [],
   "source": [
    "class NNetActivation(NNetBaseFunction):\n",
    "    def __init__(self, f=sigmoid,df=deriv_sigmoid):\n",
    "        super().__init__(f=f,df=df)\n",
    "\n",
    "    def deepcopy(self):\n",
    "        return NNetActivation(f=self.f, df=self.df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CLMLlxv99DYp"
   },
   "outputs": [],
   "source": [
    "class NNetLoss(NNetBaseFunction):\n",
    "    def __init__(self, f=squared_error,df=deriv_squared_error):\n",
    "        super().__init__(f=f,df=df)\n",
    "\n",
    "    def deepcopy(self):\n",
    "        return NNetLoss(f=self.f, df=self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qS24ybYepKNt"
   },
   "outputs": [],
   "source": [
    "class NNetMetric(NNetBaseFunction):\n",
    "    def __init__(self, f=error_rate):\n",
    "        super().__init__(f=f,df=None)\n",
    "\n",
    "    def deepcopy(self):\n",
    "        return NNetMetric(f=self.f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s8oZhAmkEKIu"
   },
   "outputs": [],
   "source": [
    "class NNetLayer:\n",
    "  def __init__(self,n_in=1,n_out=1,W=None,\n",
    "               unit=NNetActivation(), initializer=random_uniform):\n",
    "    self.n_in = n_in\n",
    "    self.n_out = n_out\n",
    "    if initializer is None:\n",
    "      initializer =  lambda n, m : np.zeros((n,m))\n",
    "    self.initializer = initializer\n",
    "    if W is None:\n",
    "      W = self.initializer(n_out,n_in+1)\n",
    "    else: \n",
    "      self.n_in, self.n_out = W.shape[1]-1, W.shape[0]\n",
    "    self.W = W\n",
    "    self.unit = unit\n",
    "\n",
    "  def ds(self, x):\n",
    "    return self.unit.df(x)\n",
    "\n",
    "  def deepcopy(self):\n",
    "    return NNetLayer(n_in=self.n_in,n_out=self.n_out,W=self.W.copy(),\n",
    "                     unit=self.unit)\n",
    "\n",
    "  def copy_layer(self, layer):\n",
    "    self.W[:] = layer.W[:]\n",
    "    return self\n",
    "\n",
    "  # assumes x[0,:] = +1\n",
    "  def aggregation_with_dummy_input(self, x):\n",
    "    return np.matmul(self.W,x)\n",
    "\n",
    "  def aggregation(self, x):\n",
    "    if x.shape[0] == self.W.shape[1]:\n",
    "      x_tmp = x\n",
    "    else: \n",
    "      x_tmp = np.ones(W.shape[1],x.shape[1])\n",
    "      x_tmp[1:,:] = x\n",
    "    return self.aggregation_with_dummy_input(x_tmp)\n",
    "\n",
    "  def activation(self, x):\n",
    "    return self.unit.f(self.aggregation(x))\n",
    "\n",
    "  def set_x(self, x):\n",
    "    return x\n",
    "\n",
    "  def set_y(self, y):\n",
    "    return y\n",
    "\n",
    "  def get_y(self):\n",
    "    return None\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lr1-1SYwf1KE"
   },
   "outputs": [],
   "source": [
    "class NNetIdentityLayer (NNetLayer):\n",
    "  def __init__(self,n_in=1,n_out=1,W=None):\n",
    "    super().__init__(n_in=n_in,n_out=n_out,W=W,\n",
    "                     unit=NNetActivation(identity,deriv_identity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q8HN1xCRfwoc"
   },
   "outputs": [],
   "source": [
    "class NNetLayerProp(NNetLayer):\n",
    "  def __init__(self,n_in=1,n_out=1,W=None,\n",
    "               unit=NNetActivation(sigmoid,deriv_sigmoid),m=1):\n",
    "    super().__init__(n_in=n_in,n_out=n_out,W=W,unit=unit)\n",
    "    # self.y = np.ones((n_out+1,m))\n",
    "    # self.y[1:,:] = 0\n",
    "    # self.delta = np.zeros((n_out+1,m))\n",
    "    self.x = None\n",
    "    self.y = None\n",
    "    self.delta = None\n",
    "\n",
    "  def deepcopy(self):\n",
    "    copy = super().deepcopy()\n",
    "    # Input is not \"stored\" by layer\n",
    "    copy.x = self.x\n",
    "    copy.y = None if self.y is None else self.y.copy()\n",
    "    copy.delta = None if self.delta is None else self.delta.copy()\n",
    "    return copy\n",
    "\n",
    "  def set_x(self, x):\n",
    "    self.x = x\n",
    "    return x\n",
    "\n",
    "  def set_y(self, y):\n",
    "    self.y = y\n",
    "    return y\n",
    "\n",
    "  def set_delta(self, delta):\n",
    "    self.delta = delta\n",
    "    return delta\n",
    "\n",
    "  def get_x(self):\n",
    "    return self.x\n",
    "\n",
    "  def get_y(self):\n",
    "    return self.y\n",
    "\n",
    "  def get_delta(self):\n",
    "    return self.delta\n",
    "\n",
    "  def dW(self):\n",
    "    return np.matmul(self.delta,self.x.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h6pa0MONvKMj"
   },
   "outputs": [],
   "source": [
    "class NNetInputLayerProp(NNetLayerProp):\n",
    "  def __init__(self,n_in=1,n_out=1,W=None,m=1):\n",
    "    super().__init__(n_in=n_in,n_out=n_out,W=W,unit=NNetActivation(identity,deriv_identity))\n",
    "    self.y = None\n",
    "\n",
    "  def deepcopy(self):\n",
    "    obj = super().deepcopy()\n",
    "    obj.y = None if self.y is None else self.y.deepcopy()\n",
    "    return obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLSx6Gy4FOmf"
   },
   "outputs": [],
   "source": [
    "class NNetOptimizer:\n",
    "  def __init__(self,loss=NNetLoss(),metric=NNetMetric()):\n",
    "    self.loss = loss\n",
    "    self.metric = metric\n",
    "    self.best_nnet = None\n",
    "    self.last_nnet = None\n",
    "    self.train_err = []\n",
    "    self.test_err = []\n",
    "    return self\n",
    "\n",
    "  def deepcopy(self):\n",
    "    opt = NNetOptimizer(loss=self.loss.deepcopy(),metric=self.metric.deepcopy())\n",
    "    opt.best_nnet = None if self.best_nnet is None else self.best_nnet.deepcopy()\n",
    "    opt.last_nnet = None if self.best_nnet is None else self.last_nnet.deepcopy()\n",
    "    opt.train_err = self.train_err.deepcopy()\n",
    "    opt.test_err = self.test_err.deepcopy()\n",
    "    return opt\n",
    "\n",
    "  def run(self,nnet,X,y):\n",
    "    return self.best_nnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_ZMiq96T460"
   },
   "outputs": [],
   "source": [
    "class NNet:\n",
    "  def __init__(self, nunits=[0,0], unit=NNetActivation(sigmoid,deriv_sigmoid), \n",
    "               output_unit=None, Layer=NNetLayerProp, InputLayer=NNetInputLayerProp):\n",
    "    self.nunits = nunits\n",
    "    self.unit = unit\n",
    "    self.output_unit = unit if output_unit is None else output_unit\n",
    "    self.nlayers = len(nunits)\n",
    "    self.layer = []\n",
    "    self.layer.append(InputLayer(n_in=1,n_out=nunits[0]))\n",
    "\n",
    "    for l in range(1,self.nlayers-1):\n",
    "      self.layer.append(Layer(n_in=nunits[l-1],n_out=nunits[l],unit=unit))\n",
    "\n",
    "    self.layer.append(Layer(n_in=nunits[-2],n_out=nunits[-1],\n",
    "                                unit=self.output_unit))\n",
    "\n",
    "  def copy(self, nnet_copy=None, Layer=NNetLayerProp, InputLayer=NNetInputLayerProp):\n",
    "    if nnet_copy is None:\n",
    "      nnet_copy = NNet(nunits=self.nunits,unit=self.unit,output_unit=self.output_unit, Layer=Layer, InputLayer=InputLayer)\n",
    "    nnet_copy.copy_layers(self)\n",
    "    return nnet_copy\n",
    "\n",
    "  def deepcopy(self, nnet_copy=None):\n",
    "    nnet_copy = self.copy(nnet_copy=nnet_copy)\n",
    "\n",
    "    nnet_copy.nunits = copy.deepcopy(self.nunits)\n",
    "    nnet_copy.unit = self.unit.deepcopy()\n",
    "    nnet_copy.output_unit = self.output_unit.deepcopy()\n",
    "\n",
    "    for l in range(1,self.nlayers):\n",
    "      nnet_copy.layer[l] = self.layer[l].deepcopy()\n",
    "\n",
    "    return nnet_copy\n",
    "\n",
    "  def copy_layers(self, nnet_copy_from):\n",
    "    for l in range(self.nlayers):\n",
    "      self.layer[l].copy_layer(nnet_copy_from.layer[l])\n",
    "    return self\n",
    "\n",
    "  def error(self, X, y, loss=squared_error, metric=None):\n",
    "    output = self.forwardprop(X.T)\n",
    "    err = np.mean(loss(y.T, output))\n",
    "    err_rate = 1.0 if metric is None else metric(y.T,output)\n",
    "    return err, err_rate\n",
    "\n",
    "  def forwardprop(self,X):\n",
    "    m = X.shape[1]\n",
    "    out_vals = np.ones((X.shape[0]+1,m))\n",
    "    out_vals[1:,:] = X\n",
    "    self.layer[0].set_y(out_vals)\n",
    "\n",
    "    for l in range(1,self.nlayers):\n",
    "      self.layer[l].set_x(self.layer[l-1].get_y())\n",
    "      del out_vals\n",
    "      out_vals = np.ones((self.nunits[l]+1,m))\n",
    "      out_vals[1:,:] = self.layer[l].activation(self.layer[l].get_x())\n",
    "      self.layer[l].set_y(out_vals)\n",
    "\n",
    "    return out_vals[1:,:]\n",
    "\n",
    "  def backprop(self,X,y,dE='deriv_squared_error'):\n",
    "    net_output = self.forwardprop(X)\n",
    "\n",
    "    layer = self.layer[self.nlayers-1]\n",
    "    layer.set_delta(layer.ds(net_output) * dE(y,net_output))\n",
    "\n",
    "    for l in range(self.nlayers-1,1,-1):\n",
    "      next_layer = self.layer[l]\n",
    "      layer = self.layer[l-1]\n",
    "      x = layer.get_y()[1:,:]\n",
    "      d = next_layer.delta\n",
    "      layer.set_delta(layer.ds(x) * np.matmul(next_layer.W[:,1:].T,d))\n",
    "\n",
    "    dW = []\n",
    "    for l in range(self.nlayers):\n",
    "      dW.append(None)\n",
    "\n",
    "    for l in range(self.nlayers-1,0,-1):\n",
    "      dW[l] = self.layer[l].dW() \n",
    "\n",
    "    return dW\n",
    "\n",
    "  def fit(self, X, y, X_test=None, y_test=None, optimizer=None, verbose=0):\n",
    "    if optimizer is None:\n",
    "      optimizer = NNetGDOptimizer(loss=NNetLoss())\n",
    "\n",
    "    best_nnet = optimizer.run(self,X,y,X_test,y_test,verbose)\n",
    "\n",
    "    self.copy_layers(best_nnet)\n",
    "\n",
    "    return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7VQNGaL9kJlo"
   },
   "outputs": [],
   "source": [
    "class NNetGDOptimizer(NNetOptimizer):\n",
    "  def __init__(self,loss=NNetLoss(),max_iters=100, learn_rate=1, reg_param=0, \n",
    "               change_thresh=1e-4, change_err_thresh=1e-6,metric=NNetMetric()):\n",
    "    super().__init__(loss=loss,metric=metric)\n",
    "    self.max_iters = max_iters\n",
    "    self.learn_rate = learn_rate\n",
    "    self.reg_param = reg_param\n",
    "    self.change_thresh = change_thresh\n",
    "    self.change_err_thresh = change_err_thresh\n",
    "\n",
    "  def deepcopy(self):\n",
    "    opt = super().deepcopy()\n",
    "    return NNetGDOptimizer(loss=opt.loss, max_iters=self.max_iters, learn_rate=self.learn_rate, reg_param=self.reg_param, \n",
    "               change_thresh=self.change_thresh, change_err_thresh=self.change_err_thresh,metric=opt.metric)\n",
    "\n",
    "  def run(self, nnet, X, y, X_test=None, y_test=None, verbose=0):\n",
    "\n",
    "    eval_test = X_test is not None and y_test is not None\n",
    "    new_nnet = NNet(nunits=nnet.nunits,unit=nnet.unit,output_unit=nnet.output_unit,Layer=NNetLayerProp,InputLayer=NNetInputLayerProp)\n",
    "    new_nnet.copy_layers(nnet)\n",
    "\n",
    "    t = 0\n",
    "    max_change = math.inf\n",
    "    min_change_err = math.inf\n",
    "\n",
    "    train_err = []\n",
    "    test_err = []\n",
    "    r = []\n",
    "\n",
    "    err, err_rate = new_nnet.error(X, y, loss=self.loss.f, metric=self.metric.f)\n",
    "    if verbose > 0:\n",
    "      print((err,err_rate))\n",
    "    min_err, min_err_rate = err, err_rate\n",
    " \n",
    "    if eval_test:\n",
    "      cv_err,cv_err_rate = new_nnet.error(X_test, y_test, loss=self.loss.f, metric=self.metric.f)\n",
    "\n",
    "    best_nnet = nnet.deepcopy()\n",
    "    best_nnet.copy_layers(new_nnet)\n",
    "\n",
    "    while min_change_err > self.change_err_thresh and max_change > self.change_thresh and t < self.max_iters:\n",
    "      if verbose > 0:\n",
    "        print(t)\n",
    "      #   print(\"Backprop...\")\n",
    "\n",
    "      dW = new_nnet.backprop(X.T, y.T,dE=self.loss.df)\n",
    "      \n",
    "      # if verbose > 0:\n",
    "      #   print(\"done.\")\n",
    "      #   print(\"Update...\")\n",
    "\n",
    "      max_change = 0\n",
    "\n",
    "      for l in range(new_nnet.nlayers-1,0,-1):\n",
    "        delta_W = self.learn_rate * (dW[l] / m + self.reg_param * new_nnet.layer[l].W)\n",
    "        new_nnet.layer[l].W[:] = new_nnet.layer[l].W[:] - delta_W[:]\n",
    "        max_change = max(max_change, np.max(np.absolute(delta_W)))\n",
    "\n",
    "      del dW[:]\n",
    "\n",
    "      # if verbose > 0:\n",
    "      #   print(\"done.\")\n",
    "\n",
    "      last_err = err\n",
    "      err,err_rate = new_nnet.error(X, y, loss=self.loss.f, metric=self.metric.f)\n",
    "      if verbose > 0:\n",
    "        print((err,err_rate))\n",
    "      min_change_err = np.absolute(err-last_err)\n",
    "\n",
    "\n",
    "      # if verbose > 0:\n",
    "      #   print(\"max_change\")\n",
    "      #   print(max_change)\n",
    "\n",
    "      if eval_test:\n",
    "        cv_err,cv_err_rate = new_nnet.error(X_test, y_test, loss=self.loss.f, metric=self.metric.f)\n",
    "\n",
    "      if verbose > 0:\n",
    "        if eval_test:\n",
    "          print(\"(test_err,test_err_rate)\")\n",
    "          print((cv_err,cv_err_rate))\n",
    "\n",
    "      if err < min_err:\n",
    "        min_err = err\n",
    "        min_err_rate = err_rate\n",
    "        best_nnet.copy_layers(new_nnet)\n",
    "        \n",
    "      r.append(t)\n",
    "      t += 1\n",
    "      \n",
    "\n",
    "      train_err.append([err, err_rate])\n",
    "      if eval_test:\n",
    "        test_err.append([cv_err, cv_err_rate])\n",
    "\n",
    "    if verbose > 0:\n",
    "      if eval_test:\n",
    "        print(\"(best_train_err,best_train_err_rate)\")\n",
    "        print((min_err,min_err_rate))\n",
    "\n",
    "    self.train_err = train_err\n",
    "    self.test_err = test_err\n",
    "    self.r = r\n",
    "\n",
    "    return best_nnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "itdITjARQyMK"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    D = np.loadtxt(\"https://raw.githubusercontent.com/nilpokharkar/ComputationalLearning/main/02/data/optdigits_train.dat\")\n",
    "    D_test = np.loadtxt(\"https://raw.githubusercontent.com/nilpokharkar/ComputationalLearning/main/02/data/optdigits_test.dat\")\n",
    "    D_trial = np.loadtxt(\"https://raw.githubusercontent.com/nilpokharkar/ComputationalLearning/main/02/data/optdigits_trial.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aVjA0wEHmJx0"
   },
   "outputs": [],
   "source": [
    "    #Training dataset transformation\n",
    "    #m is number of samples\n",
    "    #n is the size of the input data  \n",
    "    m, n = D.shape[0], D.shape[1]-1 #the shape of D is (1934, 1025) where 1024(32*32) are the pixel binary intensities of the image and 1025th index is the target variable/label/ground truth.\n",
    "\n",
    "    X = D[:,:-1].reshape(m,n) #(1934, 1024)\n",
    "    y = D[:,-1].reshape(m,1) #(1934, 1)\n",
    "\n",
    "    out_enc = LabelBinarizer() #one-hot encoding\n",
    "    y_ohe = out_enc.fit_transform(y)\n",
    "\n",
    "    K = y_ohe.shape[1] #10 - number of output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6JRp0bcFekBc"
   },
   "outputs": [],
   "source": [
    "    #Testing dataset transformation\n",
    "    m_test = D_test.shape[0]\n",
    "\n",
    "    X_test = D_test[:,:-1].reshape(m_test,n)\n",
    "    y_test = D_test[:,-1].reshape(m_test,1)\n",
    "\n",
    "    y_test_ohe = out_enc.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JiLsvyro-72P"
   },
   "outputs": [],
   "source": [
    "    #Trail dataset transformation\n",
    "    m_trial = D_trial.shape[0]\n",
    "\n",
    "    X_trial = D_trial[:,:-1].reshape(m_trial,n)\n",
    "    y_trial = D_trial[:,-1].reshape(m_trial,1)\n",
    "\n",
    "    y_trial_ohe = out_enc.transform(y_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ibmre7TCvx7"
   },
   "outputs": [],
   "source": [
    "  def nnet_error_rate(y_true, y_pred):\n",
    "    y_pred_label = np.argmax(y_pred,axis=0).reshape(-1,1)\n",
    "    y_true_label = out_enc.inverse_transform(y_true.T).reshape(-1,1)\n",
    "    return error_rate(y_true_label, y_pred_label)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQf_pYQY6FSl"
   },
   "outputs": [],
   "source": [
    "  nnet_metric = NNetMetric(f=nnet_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VqCgadcrnMqD"
   },
   "source": [
    "# Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQC3_Z3qiI0P"
   },
   "source": [
    "## Kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TLNLoM69iLmo"
   },
   "outputs": [],
   "source": [
    "  kf = KFold(n_splits = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RuVbvBtHnVyf"
   },
   "source": [
    "## Multi output perceptron\n",
    "- Input: 1024 input units\n",
    "- Output: 10 output units\n",
    "- Gradient Descent: Learning rate {4^i| i=0,1,2,3,4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wEtSRzbmiaQm",
    "outputId": "b3e01362-4813-45e2-fc81-dfff1ad73b8f"
   },
   "outputs": [],
   "source": [
    "    # Set the learning rates to be tested\n",
    "    learning_rates = [4**i for i in range(5)]\n",
    "    n = 1024\n",
    "    K = 10\n",
    "\n",
    "    nunits = make_nunits(n,K,0,0)\n",
    "    # Perform 3-fold cross validation for model selection based on misclassification error\n",
    "    cv_scores = []\n",
    "    for lr in learning_rates:\n",
    "    sum = 0\n",
    "    nnet = NNet(nunits)\n",
    "    nnet_time = time_nnet(nunits)\n",
    "\n",
    "    R = min(1000,math.ceil(MAX_TIME / (m * nnet_time)))\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        X_train, X_Val = X[train_idx], X[test_idx]\n",
    "        y_train, y_Val = y_ohe[train_idx], y_ohe[test_idx]\n",
    "\n",
    "        opt = NNetGDOptimizer(metric=nnet_metric, max_iters=R, learn_rate=lr, change_err_thresh=0, change_thresh=0)\n",
    "        best_nnet = nnet.fit(X_train,y_train,X_Val,y_Val,optimizer=opt,verbose=1)\n",
    "        err_train, err_rate_train = best_nnet.error(X_Val,y_Val, metric=nnet_error_rate)\n",
    "        sum = sum + err_train\n",
    "\n",
    "    sum = sum / 3\n",
    "    cv_scores.append(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Learning Rate calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_LUftmd7ibnb",
    "outputId": "587414c9-25d0-4c50-8ff0-8d2cf47d7606",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    # Print the cross-validation scores for each learning rate\n",
    "    for i, lr in enumerate(learning_rates):\n",
    "        print(f'Learning rate: {lr}, CV score: {cv_scores[i]}')\n",
    "\n",
    "    # Choose the best model based on the cross-validation scores\n",
    "    best_lr = learning_rates[np.argmin(cv_scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uDHjnj27s1m9",
    "outputId": "78574c02-b46c-4351-c898-9f3dffb9e833"
   },
   "outputs": [],
   "source": [
    "  best_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the multi-output perceptron using the best learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cQaN0USYnUNv",
    "outputId": "0b3a073b-6fa0-45bc-afc2-981505350fbc"
   },
   "outputs": [],
   "source": [
    "    #best_nnet = nnet.fit(X[:1000,:],y_ohe[:1000,:],X_test,y_test_ohe,optimizer=opt,verbose=1)\n",
    "    nnet = NNet(nunits)\n",
    "    opt = NNetGDOptimizer(metric=nnet_metric, max_iters=R, learn_rate=best_lr, change_err_thresh=0, change_thresh=0)\n",
    "    best_nnet = nnet.fit(X,y_ohe,X_test,y_test_ohe,optimizer=opt,verbose=1)\n",
    "    err_train, err_rate_train = best_nnet.error(X,y_ohe, metric=nnet_error_rate)\n",
    "    err_test, err_rate_test = best_nnet.error(X_test,y_test_ohe, metric=nnet_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the Proxy and Misclassification Error on the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    best_nnet01 = best_nnet\n",
    "    err_train_01, err_rate_train_01 = err_train, err_rate_train\n",
    "    err_test_01, err_rate_test_01 = err_test, err_rate_#### Calculating the Proxy and Misclassification Error on the best modeltest\n",
    "    print(\"Proxy\")\n",
    "    print(\"(train, test)\")\n",
    "    print(err_train_01, err_test_01)\n",
    "    print(\"Misclassification\")\n",
    "    print(\"(train, test)\")\n",
    "    print(err_rate_train_01, err_rate_test_01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the train and test Proxy and Misclassification error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    err_train_proxy = np.array(opt.train_err)[:,0]\n",
    "    err_test_proxy = np.array(opt.test_err)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    plt.figure(figsize=(10, 6))   \n",
    "    plt.title(\"G1: Proxy Error vs. Iterations\", size=16)\n",
    "    plt.scatter(np.array(opt.r), err_train_proxy, s=1)\n",
    "    plt.plot(np.array(opt.r), err_train_proxy, c=\"red\", label='Training')\n",
    "    plt.scatter(np.array(opt.r), err_test_proxy, s=1)\n",
    "    plt.plot(np.array(opt.r), err_test_proxy, c=\"green\", label='Testing')\n",
    "    plt.ylabel(\"Err\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    location = 0\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    err_train_misclass = np.array(opt.train_err)[:,1]\n",
    "    err_test_misclass = np.array(opt.test_err)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    plt.figure(figsize=(10, 6))   \n",
    "    plt.title(\"G2: Misclassification Error Rate vs. Iterations\", size=16)\n",
    "    plt.scatter(np.array(opt.r), err_train_misclass, s=1)\n",
    "    plt.plot(np.array(opt.r), err_train_misclass, c=\"red\", label='Train')\n",
    "    plt.scatter(np.array(opt.r), err_test_misclass, s=1)\n",
    "    plt.plot(np.array(opt.r), err_test_misclass, c=\"green\", label='Test')\n",
    "    plt.ylabel(\"Err\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    location = 0\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating on the Trial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    output = best_nnet.forwardprop(X_trial.T)\n",
    "    np.argmax(output,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNVl--R-zQJN"
   },
   "source": [
    "## Deep Neural Network\n",
    "- Number of units per hidden layer belongs to {4^2,4^3,4^4}\n",
    "- Number of hidden layers belongs to {1,2,3,4}\n",
    "- Gradient Descent: Learning rate {4^i| i=-2,-1,0,1,2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For hidden units 4^2 and layers belong to {1,2,3,4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fNrPwwfFa9ye",
    "outputId": "7f808237-01f2-4161-cfd8-fefe1986bffb",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    n_hidu = 4**2\n",
    "    # Set the learning rates to be tested\n",
    "    learning_rates = [4**i for i in range(-2,3)]\n",
    "\n",
    "    cv_scores = []\n",
    "    for i in range(1,5):\n",
    "        n_hidl = i\n",
    "        nunits = make_nunits(n,K,n_hidl,n_hidu)\n",
    "        nnet_time = time_nnet(nunits)\n",
    "\n",
    "        print('Initialize Learning')\n",
    "\n",
    "        for lr in learning_rates:\n",
    "            sum = 0\n",
    "            nnet = NNet(nunits=nunits)\n",
    "\n",
    "            R = min(1000,math.ceil(MAX_TIME / (m * nnet_time)))\n",
    "            for train_idx, test_idx in kf.split(X):\n",
    "                X_train, X_Val = X[train_idx], X[test_idx]\n",
    "                y_train, y_Val = y_ohe[train_idx], y_ohe[test_idx]\n",
    "                opt = NNetGDOptimizer(metric=nnet_metric, max_iters=R, learn_rate=lr, change_err_thresh=0, change_thresh=0)\n",
    "                best_nnet = nnet.fit(X_train,y_train,X_Val,y_Val,optimizer=opt,verbose=1)\n",
    "                err_train, err_rate_train = best_nnet.error(X_Val,y_Val,metric=nnet_error_rate)\n",
    "                sum = sum + err_train\n",
    "\n",
    "            sum = sum / 3\n",
    "            cv_scores.append(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TLupwLjeFng2",
    "outputId": "0d8024cf-5fba-4f9b-9d03-9703988299ee"
   },
   "outputs": [],
   "source": [
    "    cv_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Learning Rate and Best Layers calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J0SpR-Xd-Ch7",
    "outputId": "343715d6-aa4b-4458-8246-77546ec48b94",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    min_cv = min(cv_scores)\n",
    "    min_cv_idx = cv_scores.index(min_cv)\n",
    "    lr_idx = min_cv_idx % len(learning_rates)\n",
    "    best_lr = learning_rates[lr_idx]\n",
    "\n",
    "    Layer_idx = min_cv_idx // 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Er0ZjkTLQ0Ff",
    "outputId": "cc2da107-b92c-4b2d-c448-7ee8089b4f9d"
   },
   "outputs": [],
   "source": [
    "    Layer_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nDWiTGzUFJzi",
    "outputId": "11fbc278-27dd-4134-af5a-fa6087963141"
   },
   "outputs": [],
   "source": [
    "    best_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    best_cv_score = []\n",
    "    best_cv_score.append(min_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For hidden units 4^3 and layers belong to {1,2,3,4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bC6lT3ZPDQTC"
   },
   "outputs": [],
   "source": [
    "    n_hidu = 4**3\n",
    "    # Set the learning rates to be tested\n",
    "    learning_rates = [4**i for i in range(-2,3)]\n",
    "\n",
    "    cv_scores = []\n",
    "    for i in range(1,5):\n",
    "    n_hidl = i\n",
    "    nunits = make_nunits(n,K,n_hidl,n_hidu)\n",
    "    nnet_time = time_nnet(nunits)\n",
    "\n",
    "    print('Initialize Learning')\n",
    "\n",
    "    for lr in learning_rates:\n",
    "        sum = 0\n",
    "        nnet = NNet(nunits=nunits)\n",
    "\n",
    "        R = min(1000,math.ceil(MAX_TIME / (m * nnet_time)))\n",
    "        for train_idx, test_idx in kf.split(X):\n",
    "            X_train, X_Val = X[train_idx], X[test_idx]\n",
    "            y_train, y_Val = y_ohe[train_idx], y_ohe[test_idx]\n",
    "            opt = NNetGDOptimizer(metric=nnet_metric, max_iters=R, learn_rate=lr, change_err_thresh=0, change_thresh=0))\n",
    "            best_nnet = nnet.fit(X_train,y_train,X_Val,y_Val,optimizer=opt,verbose=1)\n",
    "            err_train, err_rate_train = best_nnet.error(X_Val,y_Val, metric=nnet_error_rate)\n",
    "            sum = sum + err_train\n",
    "\n",
    "        sum = sum / 3\n",
    "        cv_scores.append(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YCtubX37o2AU"
   },
   "outputs": [],
   "source": [
    "    cv_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Learning Rate and Best Layers calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "44AkbTzS-Q4z"
   },
   "outputs": [],
   "source": [
    "    min_cv = min(cv_scores)\n",
    "    min_cv_idx = cv_scores.index(min_cv)\n",
    "    lr_idx = min_cv_idx % len(learning_rates)\n",
    "    best_lr = learning_rates[lr_idx]\n",
    "\n",
    "    Layer_idx = min_cv_idx // 4\n",
    "    nunits = make_nunits(n,K,Layer_idx,n_hidu)\n",
    "    nnet_time = time_nnet(nunits)\n",
    "    nnet = NNet(nunits=nunits)\n",
    "\n",
    "    R = min(1000,math.ceil(MAX_TIME / (m * nnet_time)))\n",
    "\n",
    "    opt = NNetGDOptimizer(metric=nnet_metric, max_iters=R, learn_rate=best_lr, change_err_thresh=0, change_thresh=0)\n",
    "    best_nnet = nnet.fit(X,y_ohe,optimizer=opt,verbose=1)\n",
    "    err_train, err_rate_train = best_nnet.error(X_test,y_test_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y3Z2ykW2o350"
   },
   "outputs": [],
   "source": [
    "    Layer_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ycYtVIfXo6E5"
   },
   "outputs": [],
   "source": [
    "    best_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    best_cv_score.append(min_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For hidden units 4^4 and layers belong to {1,2,3,4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "050NdTi0DWwH",
    "outputId": "c0ae7688-f982-45da-9acb-e9c0b5b1c5f3"
   },
   "outputs": [],
   "source": [
    "    n_hidu = 4**4\n",
    "    # Set the learning rates to be tested\n",
    "    learning_rates = [4**i for i in range(-2,3)]\n",
    "\n",
    "    cv_scores = []\n",
    "    for i in range(1,5):\n",
    "    n_hidl = i\n",
    "    nunits = make_nunits(n,K,n_hidl,n_hidu)\n",
    "    nnet_time = time_nnet(nunits)\n",
    "\n",
    "    print('Initialize Learning')\n",
    "\n",
    "    for lr in learning_rates:\n",
    "        sum = 0\n",
    "        nnet = NNet(nunits=nunits)\n",
    "\n",
    "        R = min(1000,math.ceil(MAX_TIME / (m * nnet_time)))\n",
    "        for train_idx, test_idx in kf.split(X):\n",
    "            X_train, X_Val = X[train_idx], X[test_idx]\n",
    "            y_train, y_Val = y_ohe[train_idx], y_ohe[test_idx]\n",
    "            opt = NNetGDOptimizer(metric=nnet_metric, max_iters=R, learn_rate=lr, change_err_thresh=0, change_thresh=0)\n",
    "            best_nnet = nnet.fit(X_train,y_train,X_Val,y_Val,optimizer=opt,verbose=1)\n",
    "            err_train, err_rate_train = best_nnet.error(X_Val,y_Val, metric=nnet_error_rate)\n",
    "            sum = sum + err_train\n",
    "\n",
    "        sum = sum / 3\n",
    "        cv_scores.append(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "odJyIT-bgXwK",
    "outputId": "04cdda21-0aae-4106-bab8-88c6e8a65fe9"
   },
   "outputs": [],
   "source": [
    "    cv_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Learning Rate and Best Layers calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N0DRpXQG-ST5",
    "outputId": "204684c6-037f-4d72-922c-11f0e1e65160"
   },
   "outputs": [],
   "source": [
    "    min_cv = min(cv_scores)\n",
    "    min_cv_idx = cv_scores.index(min_cv)\n",
    "    lr_idx = min_cv_idx % len(learning_rates)\n",
    "    best_lr = learning_rates[lr_idx]\n",
    "\n",
    "    Layer_idx = min_cv_idx // 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eLmHqCmaheGZ",
    "outputId": "20f20ffe-6fa4-4710-b966-c8db64977ac1"
   },
   "outputs": [],
   "source": [
    "    Layer_idx = min_cv_idx // 4\n",
    "    Layer_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7oRtdrD7jIAb",
    "outputId": "381ad52a-74aa-4338-e23a-ce196a71e685"
   },
   "outputs": [],
   "source": [
    "    best_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    best_cv_score.append(min_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    min_cv = np.argmin(best_cv_score)\n",
    "    print(min_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the deep neural network using the best learning rate and best layers and best hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    nunits = make_nunits(1024, 10, 1 ,16)\n",
    "    nnet_time = time_nnet(nunits)\n",
    "    nnet = NNet(nunits=nunits)\n",
    "\n",
    "    R = min(1000,math.ceil(MAX_TIME / (m * nnet_time)))\n",
    "\n",
    "    opt = NNetGDOptimizer(metric=nnet_metric, max_iters=R, learn_rate=4, change_err_thresh=0, change_thresh=0)\n",
    "    best_nnet = nnet.fit(X,y_ohe,X_test,y_test_ohe,optimizer=opt,verbose=1)\n",
    "    err_train, err_rate_train = best_nnet.error(X,y_ohe,metric=nnet_error_rate)\n",
    "    err_test, err_rate_test = best_nnet.error(X_test,y_test_ohe,metric=nnet_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the train and test Proxy and Misclassification error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    err_train_proxy = np.array(opt.train_err)[:,0]\n",
    "    err_test_proxy = np.array(opt.test_err)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    plt.figure(figsize=(10, 6))   \n",
    "    plt.title(\"G1: Proxy Error vs. Iterations\", size=16)\n",
    "    plt.scatter(np.array(opt.r), err_train_proxy, s=1)\n",
    "    plt.plot(np.array(opt.r), err_train_proxy, c=\"red\", label='Training')\n",
    "    plt.scatter(np.array(opt.r), err_test_proxy, s=1)\n",
    "    plt.plot(np.array(opt.r), err_test_proxy, c=\"green\", label='Test')\n",
    "    plt.ylabel(\"Err\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    location = 0\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    err_train_misclass = np.array(opt.train_err)[:,1]\n",
    "    err_test_misclass = np.array(opt.test_err)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    plt.figure(figsize=(10, 6))   \n",
    "    plt.title(\"G2: Misclassification Error Rate vs. Iterations\", size=16)\n",
    "    plt.scatter(np.array(opt.r), err_train_misclass, s=1)\n",
    "    plt.plot(np.array(opt.r), err_train_misclass, c=\"red\", label='Training')\n",
    "    plt.scatter(np.array(opt.r), err_test_misclass, s=1)\n",
    "    plt.plot(np.array(opt.r), err_test_misclass, c=\"green\", label='Test')\n",
    "    plt.ylabel(\"Err\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    location = 0\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating on the Trial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    output = best_nnet.forwardprop(X_trial.T)\n",
    "    np.argmax(output,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the Proxy and Misclassification Error on the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9b5ZH-SGpa2w"
   },
   "outputs": [],
   "source": [
    "    print('Proxy')\n",
    "    print('(train, test)')\n",
    "    print(err_train, err_test)\n",
    "    print('Misclassification')\n",
    "    print('(train, test)')\n",
    "    print(err_rate_train, err_rate_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFlVXliH6Lz9"
   },
   "source": [
    "## Deep Neural Network with 2 hidden layer\n",
    "- Number of units in 1st hidden layer belongs to {4^3,4^4}\n",
    "- Smaller number of units in 2nd hidden layer belongs to {4^2,4^3}\n",
    "- Gradient Descent: Learning rate {4^i| i=-3,-2,-2,0,1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For 1st hidden layer units: 4^3 and 2nd hidden layer units: 4^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TWjKMoNy6sgN",
    "outputId": "b7e029de-b059-43f5-ac79-5c75e2a7f8d3"
   },
   "outputs": [],
   "source": [
    "    learning_rates = [4**i for i in range(-3,2)]\n",
    "\n",
    "    cv_scores = []\n",
    "    nunits = [1024, 4**3, 4**2, 10]\n",
    "\n",
    "    print('Initialize Learning')\n",
    "\n",
    "    for lr in learning_rates:\n",
    "        sum = 0\n",
    "        nnet = NNet(nunits)\n",
    "        nnet_time = time_nnet(nunits)\n",
    "\n",
    "        R = min(1000,math.ceil(MAX_TIME / (m * nnet_time)))\n",
    "        for train_idx, test_idx in kf.split(X):\n",
    "            X_train, X_Val = X[train_idx], X[test_idx]\n",
    "            y_train, y_Val = y_ohe[train_idx], y_ohe[test_idx]\n",
    "            opt = NNetGDOptimizer(metric=nnet_metric, max_iters=R, learn_rate=lr, change_err_thresh=0, change_thresh=0)\n",
    "            best_nnet = nnet.fit(X_train,y_train,X_Val,y_Val,optimizer=opt,verbose=1)\n",
    "            err_train, err_rate_train = best_nnet.error(X_Val,y_Val, metric=nnet_error_rate)\n",
    "            sum = sum + err_train\n",
    "\n",
    "        sum = sum / 3\n",
    "        cv_scores.append(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "guOE5tcepgZy",
    "outputId": "712758a6-eb3e-476c-c1bc-9fe1bbcc1d2b"
   },
   "outputs": [],
   "source": [
    "    cv_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Learning Rate and Best Layers calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7KGsYkgk-Uoc"
   },
   "outputs": [],
   "source": [
    "    min_cv = min(cv_scores)\n",
    "    min_cv_idx = cv_scores.index(min_cv)\n",
    "    lr_idx = min_cv_idx % len(learning_rates)\n",
    "    best_lr = learning_rates[lr_idx]\n",
    "\n",
    "    Layer_idx = min_cv_idx // 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OVmZcS8XpmN0",
    "outputId": "c9f43044-92c5-4fe7-8b5d-f197a723875d"
   },
   "outputs": [],
   "source": [
    "    Layer_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GummRetLpj_n",
    "outputId": "1741d853-24ab-4f25-d5ad-1ba9367759af"
   },
   "outputs": [],
   "source": [
    "    best_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    best_cv_score = []\n",
    "    best_cv_score.append(min_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For 1st hidden layer units: 4^4 and 2nd hidden layer units: 4^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "LhGtnzYTmqDo",
    "outputId": "967ad401-b390-4c49-e6b5-ae103ce0682c"
   },
   "outputs": [],
   "source": [
    "    learning_rates = [4**i for i in range(-3,2)]\n",
    "\n",
    "    cv_scores = []\n",
    "    nunits = [1024, 4**4, 4**2, 10]\n",
    "\n",
    "    print('Initialize Learning')\n",
    "\n",
    "    for lr in learning_rates:\n",
    "        sum = 0\n",
    "        nnet = NNet(nunits)\n",
    "        nnet_time = time_nnet(nunits)\n",
    "\n",
    "        R = min(1000,math.ceil(MAX_TIME / (m * nnet_time)))\n",
    "        for train_idx, test_idx in kf.split(X):\n",
    "            X_train, X_Val = X[train_idx], X[test_idx]\n",
    "            y_train, y_Val = y_ohe[train_idx], y_ohe[test_idx]\n",
    "            opt = NNetGDOptimizer(metric=nnet_metric, max_iters=R, learn_rate=lr, change_err_thresh=0, change_thresh=0)\n",
    "            best_nnet = nnet.fit(X_train,y_train,X_Val,y_Val,optimizer=opt,verbose=1)\n",
    "            err_train, err_rate_train = best_nnet.error(X_Val,y_Val, metric=nnet_error_rate)\n",
    "            sum = sum + err_train\n",
    "\n",
    "        sum = sum / 3\n",
    "        cv_scores.append(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "FjChCBaCpz13",
    "outputId": "afb0ffb4-c2a1-4ea0-e2c3-bcf0989a0ab1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    cv_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Learning Rate and Best Layers calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "TPhGKVFK-XbZ"
   },
   "outputs": [],
   "source": [
    "    min_cv = min(cv_scores)\n",
    "    min_cv_idx = cv_scores.index(min_cv)\n",
    "    lr_idx = min_cv_idx % len(learning_rates)\n",
    "    best_lr = learning_rates[lr_idx]\n",
    "\n",
    "    Layer_idx = min_cv_idx // 4\n",
    "    best_cv_score.append(min_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "HdP3dyrep2NJ",
    "outputId": "d6fe6f2e-6fea-45cf-8902-b53816a6faab"
   },
   "outputs": [],
   "source": [
    "    best_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "hwUGtyJlp6l6",
    "outputId": "ef110f16-657a-4051-9dc2-ef7a1f462272"
   },
   "outputs": [],
   "source": [
    "    Layer_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For 1st hidden layer units: 4^4 and 2nd hidden layer units: 4^3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Kwhlh9LLmx_e",
    "outputId": "ce06cf64-4480-4b4c-d3f9-33c0684718f8"
   },
   "outputs": [],
   "source": [
    "    learning_rates = [4**i for i in range(-3,2)]\n",
    "\n",
    "    cv_scores = []\n",
    "    nunits = [1024, 4**4, 4**3, 10]\n",
    "\n",
    "    print('Initialize Learning')\n",
    "\n",
    "    for lr in learning_rates:\n",
    "        sum = 0\n",
    "        nnet = NNet(nunits)\n",
    "        nnet_time = time_nnet(nunits)\n",
    "\n",
    "        R = min(1000,math.ceil(MAX_TIME / (m * nnet_time)))\n",
    "        for train_idx, test_idx in kf.split(X):\n",
    "            X_train, X_Val = X[train_idx], X[test_idx]\n",
    "            y_train, y_Val = y_ohe[train_idx], y_ohe[test_idx]\n",
    "            opt = NNetGDOptimizer(metric=nnet_metric, max_iters=R, learn_rate=lr, change_err_thresh=0, change_thresh=0)\n",
    "            best_nnet = nnet.fit(X_train,y_train,X_Val,y_Val,optimizer=opt,verbose=1)\n",
    "            err_train, err_rate_train = best_nnet.error(X_Val,y_Val, metric=nnet_error_rate)\n",
    "            sum = sum + err_train\n",
    "\n",
    "        sum = sum / 3\n",
    "        cv_scores.append(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "K2YNr6npp8pg",
    "outputId": "422f9e6d-bd6c-476e-a93e-6d7e646ddf78"
   },
   "outputs": [],
   "source": [
    "    cv_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Learning Rate and Best Layers calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "LXxM9Ydt-YlF"
   },
   "outputs": [],
   "source": [
    "    min_cv = min(cv_scores)\n",
    "    min_cv_idx = cv_scores.index(min_cv)\n",
    "    lr_idx = min_cv_idx % len(learning_rates)\n",
    "    best_lr = learning_rates[lr_idx]\n",
    "\n",
    "    Layer_idx = min_cv_idx // 4\n",
    "    best_cv_score.append(min_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "OCZKR0J_p-xK",
    "outputId": "5b77a8ef-4300-4754-ea90-101b64dd3b7e"
   },
   "outputs": [],
   "source": [
    "    best_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    min_cv = np.argmin(best_cv_score)\n",
    "    print(min_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the deep neural network using the best learning rate and best hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    nunits = [1024, 4**4, 4**3, 10]\n",
    "    nnet = NNet(nunits)\n",
    "    nnet_time = time_nnet(nunits)\n",
    "\n",
    "    R = min(1000,math.ceil(MAX_TIME / (m * nnet_time)))\n",
    "    opt = NNetGDOptimizer(metric=nnet_metric, max_iters=R, learn_rate=4, change_err_thresh=0, change_thresh=0)\n",
    "    best_nnet = nnet.fit(X,y_ohe,X_test,y_test_ohe,optimizer=opt,verbose=1)\n",
    "    err_train, err_rate_train = best_nnet.error(X,y_ohe, metric=nnet_error_rate)\n",
    "    err_test, err_rate_test = best_nnet.error(X_test,y_test_ohe, metric=nnet_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the train and test Proxy and Misclassification error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    err_train_proxy = np.array(opt.train_err)[:,0]\n",
    "    err_test_proxy = np.array(opt.test_err)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    plt.figure(figsize=(10, 6))   \n",
    "    plt.title(\"G1: Proxy Error vs. Iterations\", size=16)\n",
    "    plt.scatter(np.array(opt.r), err_train_proxy, s=1)\n",
    "    plt.plot(np.array(opt.r), err_train_proxy, c=\"red\", label='Training')\n",
    "    plt.scatter(np.array(opt.r), err_test_proxy, s=1)\n",
    "    plt.plot(np.array(opt.r), err_test_proxy, c=\"green\", label='Testing')\n",
    "    plt.ylabel(\"Err\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    location = 0\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    err_train_misclass = np.array(opt.train_err)[:,1]\n",
    "    err_test_misclass = np.array(opt.test_err)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    plt.figure(figsize=(10, 6))   \n",
    "    plt.title(\"G2: Misclassification Error Rate vs. Iterations\", size=16)\n",
    "    plt.scatter(np.array(opt.r), err_train_misclass, s=1)\n",
    "    plt.plot(np.array(opt.r), err_train_misclass, c=\"red\", label='Training')\n",
    "    plt.scatter(np.array(opt.r), err_test_misclass, s=1)\n",
    "    plt.plot(np.array(opt.r), err_test_misclass, c=\"green\", label='Testing')\n",
    "    plt.ylabel(\"Err\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    location = 0\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating on the Trial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    output = best_nnet.forwardprop(X_trial.T)\n",
    "    np.argmax(output,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    best_nnet_03 = best_nnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the Proxy and Misclassification Error on the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    err_train_03, err_rate_train_03 = err_train, err_rate_train\n",
    "    err_test_03, err_rate_test_03 = err_test, err_rate_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print('Proxy')\n",
    "    print('(train, test)')\n",
    "    print(err_train_03, err_test_03)\n",
    "    print('Misclassification')\n",
    "    print('(train, test)')\n",
    "    print(err_rate_train_03, err_rate_test_03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
